# File: core/router.py (C·∫¨P NH·∫¨T - FIX RAG)

import os
from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableBranch, RunnableLambda, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.documents import Document

# Import c√°c chain con v√† retriever
from .chains.full_plan_chain import full_plan_chain    # LU·ªíNG 2 (L√™n k·∫ø ho·∫°ch)
from .chains.prompts import router_prompt, rag_direct_prompt
from .chains.retriever import retriever 

# <<< IMPORT LU·ªíNG M·ªöI (LU·ªíNG 3) >>>
from .agents.executor import agent_executor           # LU·ªíNG 3 (Th·ª±c thi)

load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("GEMINI_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y")

# LLM cho router ph√¢n lo·∫°i (Flash)
router_llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash",
                                  google_api_key=api_key)

# LLM cho vi·ªác t·∫°o c√¢u tr·∫£ l·ªùi cu·ªëi c√πng (Pro)
answer_llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash",
                                  google_api_key=api_key,
                                  temperature=0.3)

# H√†m helper ƒë·ªÉ ƒë·ªãnh d·∫°ng context t·ª´ retriever
def format_docs(docs: list[Document]) -> str:
    if not isinstance(docs, list) or not docs:
        print("üî¥ [RAG DEBUG] Kh√¥ng t√¨m th·∫•y documents n√†o!")
        return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong c∆° s·ªü tri th·ª©c."
    
    top_k_docs = docs[:5]  # L·∫•y top 5 thay v√¨ 3
    
    # === DEBUG LOGGING ===
    print(f"\n{'='*60}")
    print(f"üü¢ [RAG DEBUG] T√¨m th·∫•y {len(docs)} documents, l·∫•y top {len(top_k_docs)}")
    for i, doc in enumerate(top_k_docs):
        source = doc.metadata.get('source', 'N/A')
        content_preview = doc.page_content[:200].replace('\n', ' ')
        print(f"  üìÑ Doc {i+1}: {source}")
        print(f"     Preview: {content_preview}...")
    print(f"{'='*60}\n")
    # === END DEBUG ===
    
    return "\n\n---\n\n".join(
        f"Ngu·ªìn: {doc.metadata.get('source', 'N/A')}\n\n{doc.page_content}"
        for doc in top_k_docs
    )

# H√†m helper ƒë·ªÉ tr√≠ch xu·∫•t user_input cho c√°c chain con
def prepare_subchain_input(input_dict: dict) -> dict:
    return {"user_input": input_dict["user_input"]}

# H√†m debug ƒë·ªÉ log th√¥ng tin ph√¢n lo·∫°i
def log_classification(input_dict: dict) -> dict:
    topic = input_dict.get("topic", "UNKNOWN")
    user_input = input_dict.get("user_input", "")[:50]
    rag_docs = input_dict.get("rag_context_docs", [])
    
    print(f"\n{'='*60}")
    print(f"üîµ [ROUTER DEBUG] Topic: {topic}")
    print(f"   User Input: {user_input}...")
    print(f"   RAG Docs Count: {len(rag_docs) if rag_docs else 0}")
    print(f"{'='*60}\n")
    
    return input_dict

# Helper ƒë·ªÉ check c√≥ docs hay kh√¥ng
def has_rag_docs(x):
    docs = x.get("rag_context_docs")
    return docs is not None and len(docs) > 0

# Chain RAG Tr·ª±c ti·∫øp (LU·ªíNG 1)
direct_rag_answer_chain = (
    # Nh·∫≠n input {"user_input": ..., "rag_context": ...}
    rag_direct_prompt
    | answer_llm
    | StrOutputParser()
)

def create_router():
    """
    T·∫°o Router Chain th√¥ng minh: 
    Ph√¢n lo·∫°i -> Ki·ªÉm tra RAG -> Ch·ªçn 1 trong 3 Lu·ªìng.
    """
    
    # 1. Chain ph√¢n lo·∫°i √Ω ƒë·ªãnh
    # Input: {"user_input": "..."} -> Output: string (topic)
    classifier_chain = (lambda x: x["user_input"]) | router_prompt | router_llm | StrOutputParser()

    # 2. Chain L·∫•y Context RAG S·ªõm
    # Input: {"user_input": "..."} -> Output: list[Document]
    early_rag_retrieval_chain = (lambda x: x["user_input"]) | retriever

    # 3. Logic Ph√¢n nh√°nh 3 Lu·ªìng (C·∫¨P NH·∫¨T - S·ª¨A L·ªñI RAG)
    # Input cho branch l√† dict: {"topic": ..., "user_input": ..., "rag_context_docs": ...}
    branch = RunnableBranch(
        
        # ƒêI·ªÄU KI·ªÜN 1: N·∫øu l√† y√™u c·∫ßu TH·ª∞C THI (LU·ªíNG 3)
        # ∆Øu ti√™n cao nh·∫•t
        (lambda x: "execute_pentest_tool" in x["topic"],
            # Ch·∫°y Agent Executor
            RunnableLambda(prepare_subchain_input) | agent_executor
        ),
        
        # ƒêI·ªÄU KI·ªÜN 2: N·∫øu l√† c√¢u h·ªèi c·ª• th·ªÉ V·ªÄ VULNERABILITY ho·∫∑c TOOL (LU·ªíNG 1)
        # Lu√¥n d√πng RAG chain cho c√°c c√¢u h·ªèi n√†y, c√≥ ho·∫∑c kh√¥ng c√≥ docs
        (lambda x: "specific_vulnerability_info" in x["topic"] or "tool_usage" in x["topic"],
            # ƒê·ªãnh d·∫°ng context v√† ch·∫°y chain RAG TR·ª∞C TI·∫æP
            RunnableLambda(
                lambda x: {
                    "user_input": x["user_input"],
                    "rag_context": format_docs(x.get("rag_context_docs", []))
                }
            ) | direct_rag_answer_chain
        ),
        
        # FALLBACK: (LU·ªíNG 2 - L√™n k·∫ø ho·∫°ch)
        # N·∫øu l√† 'generate_full_plan' HO·∫∂C c√°c lu·ªìng kia kh√¥ng kh·ªõp
        RunnableLambda(prepare_subchain_input) | full_plan_chain
    )

    # 4. G·∫Øn k·∫øt t·∫•t c·∫£ l·∫°i
    # - Nh·∫≠n input {"user_input": "..."}
    # - Ch·∫°y classifier l·∫•y "topic"
    # - Ch·∫°y retriever s·ªõm l·∫•y "rag_context_docs"
    # - ƒê∆∞a c·∫£ ba v√†o chain ph√¢n nh√°nh 'branch'
    final_chain = RunnablePassthrough.assign(
        topic=classifier_chain,
        rag_context_docs=early_rag_retrieval_chain
    ) | RunnableLambda(log_classification) | branch

    return final_chain