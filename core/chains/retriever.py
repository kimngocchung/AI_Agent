# File: core/chains/retriever.py (Phi√™n b·∫£n v·ªõi Source Filtering - IMPROVED)

import os
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.runnables import RunnableLambda

def get_index_path():
    """Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi t·ªõi FAISS index"""
    base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    return os.path.join(base_dir, 'my_faiss_index')

INDEX_DIRECTORY = get_index_path()

# Bi·∫øn global ƒë·ªÉ cache
_embeddings = None
_vectorstore = None
_last_doc_count = 0

def get_embeddings():
    """Lazy load embeddings model"""
    global _embeddings
    if _embeddings is None:
        print("--- [RAG] ƒêang kh·ªüi t·∫°o model embedding... ---")
        try:
            _embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
            print("--- [RAG] Model embedding ƒë√£ s·∫µn s√†ng! ---")
        except Exception as e:
            print(f"--- [RAG Error] L·ªói kh·ªüi t·∫°o embedding: {e} ---")
            raise
    return _embeddings

def load_vectorstore(force_reload=False):
    """Load ho·∫∑c reload vectorstore"""
    global _vectorstore, _last_doc_count
    
    index_path = get_index_path()
    embeddings = get_embeddings()
    
    current_doc_count = 0
    if os.path.exists(index_path):
        try:
            temp_vs = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)
            current_doc_count = temp_vs.index.ntotal
        except:
            pass
    
    need_reload = (
        _vectorstore is None or 
        force_reload or 
        current_doc_count != _last_doc_count
    )
    
    if need_reload:
        if current_doc_count != _last_doc_count and _last_doc_count > 0:
            print(f"--- [RAG] Ph√°t hi·ªán thay ƒë·ªïi: {_last_doc_count} -> {current_doc_count} docs ---")
        
        print(f"--- [RAG] Loading vectorstore t·ª´: {index_path} ---")
        
        if not os.path.exists(index_path):
            print(f"--- [RAG Warning] Kh√¥ng t√¨m th·∫•y index, t·∫°o r·ªóng ---")
            _vectorstore = FAISS.from_texts(["Ch∆∞a c√≥ ngu·ªìn d·ªØ li·ªáu."], embeddings)
            _last_doc_count = 1
        else:
            try:
                _vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)
                _last_doc_count = _vectorstore.index.ntotal
                print(f"--- [RAG] Loaded {_last_doc_count} documents ---")
            except Exception as e:
                print(f"--- [RAG Error] Load failed: {e} ---")
                _vectorstore = FAISS.from_texts(["L·ªói t·∫£i index"], embeddings)
                _last_doc_count = 1
    
    return _vectorstore


def normalize_source_name(name: str) -> str:
    """Chu·∫©n h√≥a t√™n ngu·ªìn ƒë·ªÉ so s√°nh"""
    if not name:
        return ""
    # L·∫•y ph·∫ßn cu·ªëi c·ªßa path/URL n·∫øu c√≥
    name = name.lower().strip()
    # Lo·∫°i b·ªè http/https
    name = name.replace("https://", "").replace("http://", "")
    # Lo·∫°i b·ªè trailing slashes
    name = name.rstrip("/")
    return name


def source_matches(doc_source: str, selected_names: list) -> bool:
    """Ki·ªÉm tra xem document source c√≥ match v·ªõi selected sources kh√¥ng"""
    if not selected_names:
        return True  # Kh√¥ng c√≥ filter = match t·∫•t c·∫£
    
    doc_normalized = normalize_source_name(doc_source)
    
    for sel_name in selected_names:
        sel_normalized = normalize_source_name(sel_name)
        
        # Exact match
        if doc_normalized == sel_normalized:
            return True
        
        # Partial match: ki·ªÉm tra doc source c√≥ ch·ª©a trong selected name ho·∫∑c ng∆∞·ª£c l·∫°i
        if doc_normalized and sel_normalized:
            if doc_normalized in sel_normalized or sel_normalized in doc_normalized:
                return True
            
            # Match by basename (for URLs and file paths)
            doc_parts = doc_normalized.split("/")
            sel_parts = sel_normalized.split("/")
            
            # So s√°nh domain ho·∫∑c file basename
            if doc_parts and sel_parts:
                # N·∫øu doc_source ng·∫Øn (nh∆∞ 'cve'), ki·ªÉm tra xem n√≥ c√≥ trong selected_name
                if len(doc_normalized) < 20:
                    for part in sel_parts:
                        if doc_normalized == part or doc_normalized in part:
                            return True
    
    return False


def extract_cve_from_query(query: str) -> list:
    """Tr√≠ch xu·∫•t CVE IDs t·ª´ query"""
    import re
    cve_pattern = re.compile(r'CVE-\d{4}-\d{4,}', re.IGNORECASE)
    return cve_pattern.findall(query.upper())


def retrieve_docs_with_filter(input_data) -> list:
    """
    Retrieve documents v·ªõi filter theo selected sources.
    Input c√≥ th·ªÉ l√† string (query) ho·∫∑c dict {query, selected_sources}
    """
    # Parse input
    if isinstance(input_data, str):
        query = input_data
        selected_sources = None
    elif isinstance(input_data, dict):
        query = input_data.get("user_input", input_data.get("query", ""))
        selected_sources = input_data.get("selected_sources", None)
    else:
        query = str(input_data)
        selected_sources = None
    
    vs = load_vectorstore()
    
    print(f"--- [RAG] Searching for: {query[:50]}... ---")
    
    # 1. T√åM CVE CH√çNH X√ÅC TR∆Ø·ªöC (n·∫øu query c√≥ CVE ID)
    cve_ids = extract_cve_from_query(query)
    cve_matched_docs = []
    
    if cve_ids:
        print(f"--- [RAG] Detected CVE IDs: {cve_ids} ---")
        
        # L·∫•y t·∫•t c·∫£ documents v√† t√¨m exact match CVE
        all_docs = vs.similarity_search(query, k=100)  # L·∫•y nhi·ªÅu ƒë·ªÉ t√¨m CVE
        
        for doc in all_docs:
            doc_source = doc.metadata.get('source', '').upper()
            doc_content = doc.page_content.upper()[:500]
            
            for cve_id in cve_ids:
                if cve_id in doc_source or cve_id in doc_content:
                    if doc not in cve_matched_docs:
                        cve_matched_docs.append(doc)
                        print(f"    üéØ CVE MATCH: {doc.metadata.get('source', '')[:70]}")
        
        if cve_matched_docs:
            print(f"--- [RAG] Found {len(cve_matched_docs)} docs matching CVE IDs ---")
    
    # 2. SIMILARITY SEARCH FALLBACK
    k_fetch = 30 if selected_sources else 10
    similarity_docs = vs.similarity_search(query, k=k_fetch)
    
    # 3. MERGE RESULTS: CVE matches first, then similarity
    docs = cve_matched_docs.copy()
    for doc in similarity_docs:
        if doc not in docs:
            docs.append(doc)
    
    # 4. Filter theo selected sources n·∫øu c√≥
    if selected_sources and len(selected_sources) > 0:
        selected_names = [s.get('name', '') for s in selected_sources]
        
        print(f"--- [RAG] Filtering by {len(selected_names)} selected sources ---")
        for name in selected_names:
            print(f"    ‚úì Selected: {name[:60]}")
        
        # Filter docs
        filtered_docs = []
        rejected_docs = []
        
        for doc in docs:
            doc_source = doc.metadata.get('source', '')
            
            if source_matches(doc_source, selected_names):
                filtered_docs.append(doc)
                print(f"    ‚úÖ MATCH: {doc_source[:60]}")
            else:
                rejected_docs.append(doc_source)
        
        # Log rejected docs for debugging
        if rejected_docs:
            print(f"    ‚ùå REJECTED {len(rejected_docs)} docs:")
            for rej in rejected_docs[:3]:
                print(f"       - {rej[:60]}")
        
        docs = filtered_docs[:5]  # L·∫•y top 5 sau khi filter
        print(f"--- [RAG] After filter: {len(docs)} documents ---")
        
        # N·∫øu kh√¥ng c√≥ docs match, log warning
        if not docs:
            print("--- [RAG WARNING] Kh√¥ng c√≥ document n√†o match v·ªõi selected sources! ---")
    else:
        docs = docs[:5]
        print(f"--- [RAG] No filter applied, using all sources ---")
    
    print(f"--- [RAG] Found {len(docs)} documents ---")
    for i, doc in enumerate(docs[:5]):
        source = doc.metadata.get('source', 'N/A')
        print(f"    {i+1}. {source[:80]}")
    
    return docs


def retrieve_docs(query: str) -> list:
    """H√†m retrieve documents c≈© - compatibility"""
    return retrieve_docs_with_filter(query)


def reload_retriever():
    """Force reload - g·ªçi sau khi th√™m ngu·ªìn m·ªõi"""
    global _vectorstore, _last_doc_count
    print("--- [RAG] Force reload... ---")
    _vectorstore = None
    _last_doc_count = 0
    load_vectorstore(force_reload=True)

def create_retriever():
    """T·∫°o retriever object cho LangChain compatibility"""
    vs = load_vectorstore()
    return vs.as_retriever(search_kwargs={"k": 5})

# === EXPORT ===
retriever = RunnableLambda(retrieve_docs_with_filter)